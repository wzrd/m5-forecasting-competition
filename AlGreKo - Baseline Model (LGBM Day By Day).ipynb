{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ILV68NGScnf"
   },
   "source": [
    "# 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2356,
     "status": "ok",
     "timestamp": 1589283888108,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "HKH9EkudScng",
    "outputId": "7955164d-1edd-4913-e586-588cad4feb5b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "from pylab import rcParams\n",
    "import time\n",
    "from  datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set()\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# # Google Colab trick to extend memory\n",
    "# a = []\n",
    "# while(1):\n",
    "#     a.append('1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WjlAJnT2Scnm"
   },
   "source": [
    "## 1.1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9AbC-zQIScnm"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def display_missing(df):    \n",
    "    for col in df.columns.tolist():  \n",
    "        if df[col].isnull().sum() != 0:\n",
    "            print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ti6p45VScnq"
   },
   "source": [
    "## 1.2 Loading data grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2343,
     "status": "ok",
     "timestamp": 1589283888110,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "RwWG8OiRpf_l",
    "outputId": "9773c2c9-b6cd-4353-8f59-8968efafea35"
   },
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4029,
     "status": "ok",
     "timestamp": 1589283889820,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "jNE6Zy8zScnq",
    "outputId": "06c81906-a358-4c11-930f-35cabdd9ad8c"
   },
   "outputs": [],
   "source": [
    "# Setting directories where data is stored and ouptut dir\n",
    "if IN_COLAB:\n",
    "    DATA_GRID_INPUT_DIR = './drive/My Drive/Colab Notebooks' \n",
    "    DATA_OUTPUT_DIR = './drive/My Drive/Colab Notebooks'\n",
    "    !ls './drive/My Drive/Colab Notebooks'\n",
    "else:\n",
    "    DATA_GRID_INPUT_DIR = '.'\n",
    "    DATA_OUTPUT_DIR = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51565,
     "status": "ok",
     "timestamp": 1589283937390,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "UtLS2plBScnt",
    "outputId": "7b798613-a90c-462a-d85b-9e2a31e5f426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading the data...')\n",
    "\n",
    "data = pd.read_pickle(f'{DATA_GRID_INPUT_DIR}/m5_data_model2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNYreOEtScnw"
   },
   "source": [
    "## 1.3 Init variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0iuKum57Scnw"
   },
   "outputs": [],
   "source": [
    "h = 28 # Prediction horizon\n",
    "max_lags = 120 # Max lags used\n",
    "TRAINING_LAST_DAY_NUM = 1913 # Last day for training data\n",
    "FIRST_PRED_DAY = datetime(2016,4, 25) # First prediction day\n",
    "FIRST_LOADING_DAY = datetime(2013, 4,7) # First day for training\n",
    "FIRST_LOADING_DAY_NUM = 800\n",
    "SEED = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3_SRy3RScn3"
   },
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQIkNyyEScn4"
   },
   "source": [
    "## Creating features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FZXdWPSv9hd"
   },
   "outputs": [],
   "source": [
    "# data = data.loc[data.date > '2015-01-01'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BeAoryHtwFBS"
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yjdkw71jv0b2"
   },
   "outputs": [],
   "source": [
    "# dept_store_df = data[['id', 'date', 'item_id', 'dept_id', 'store_id', 'sales']].groupby(['dept_id', 'store_id', 'date'])['sales'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_jVVBq0wEdw"
   },
   "outputs": [],
   "source": [
    "# dept_store_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23U28rm2shvO"
   },
   "outputs": [],
   "source": [
    "# dept_store_df['dept_store_lag_3'] = dept_store_df.groupby(['dept_id', 'store_id'])['sales'].shift(28)\n",
    "# dept_store_df['dept_sotre_rmean_3_3'] = dept_store_df.groupby(['dept_id', 'store_id'])['dept_store_lag_3'].transform(lambda x: x.rolling(3).mean())\n",
    "# dept_store_df.drop(['sales'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziP0x2bQ1mwU"
   },
   "outputs": [],
   "source": [
    "# dept_store_df.loc[dept_store_df.date=='2016-04-23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1PEVYdY2EnB"
   },
   "outputs": [],
   "source": [
    "# data = data.merge(dept_store_df, on=['dept_id', 'store_id', 'date'], copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "866wfXsB3WzP"
   },
   "outputs": [],
   "source": [
    "# data.loc[data.date=='2016-04-23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTWdgH6ZScn4"
   },
   "outputs": [],
   "source": [
    "# Init global variable to store columns of encodings\n",
    "# To use them later for test dataset\n",
    "\n",
    "# target_enc_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lESYikvOScn7"
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "\n",
    "#     agg_levels = [['dept_id', 'store_id'],\n",
    "#                   ['item_id']\n",
    "#                   ]\n",
    "#     agg_level_names = ['_'.join(level) for level in agg_levels]\n",
    "    \n",
    "#     # Create dataframes grouped by agg_levels and date\n",
    "#     agg_df = dict()\n",
    "#     for level, level_name in zip(agg_levels, agg_level_names):\n",
    "#         agg_df[level_name] = df[['id', 'date', 'item_id', 'dept_id', 'store_id', 'sales']].groupby(level + ['date'])['sales'].sum().reset_index()\n",
    "\n",
    "    lags = [1, 7, 14, 28]\n",
    "    lag_cols = [f\"lag_t{lag}\" for lag in lags ]\n",
    "  \n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        df[lag_col] = df[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag).astype(np.float16)\n",
    "        \n",
    "#         # Setting lags for agg levels\n",
    "#         if lag in [1, 7, 28]:\n",
    "#             for level, level_name in zip(agg_levels, agg_level_names):\n",
    "#                 agg_df[level_name][level_name + '_' + lag_col] = agg_df[level_name].groupby(level)['sales'].shift(lag)\n",
    "\n",
    "    wins = [7, 14, 28, 60]\n",
    "    for win in wins:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            df[f\"rmean_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean()).astype(np.float16)\n",
    "\n",
    "            df[f\"rmedian_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).median()).astype(np.float16)\n",
    "            # df[f\"rdiff_mean_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).diff().mean()).astype(np.float16)\n",
    "            df[f\"rstd_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).std()).astype(np.float16)\n",
    "            df[f'rmean_{lag}_{win}_decay'] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x: x.ewm(span=win).mean()).astype(np.float16)\n",
    "\n",
    "#             # Computing rollings for aggregation levels\n",
    "#             if (lag in [1, 7, 28]) and (win in [7, 28]):\n",
    "#                 for level, level_name in zip(agg_levels, agg_level_names):\n",
    "#                     agg_df[level_name][level_name + '_' + f'rmean_{lag}_{win}'] = agg_df[level_name].groupby(level)[level_name + '_' + lag_col].transform(lambda x: x.rolling(win).mean())\n",
    "            \n",
    "            \n",
    "\n",
    "#     # Merging agg levels computations with main grid\n",
    "#     for level, level_name in zip(agg_levels, agg_level_names):\n",
    "#         agg_df[level_name].drop(['sales'], axis=1, inplace=True)\n",
    "#         df = df.merge(agg_df[level_name], on=level + ['date'], copy=False)\n",
    "\n",
    "    df['price_mean_t60'] = df[['id','sell_price']].groupby([\"id\"])[\"sell_price\"].transform(lambda x: x.rolling(60).mean()).astype(np.float16)\n",
    "    df['price_momentum_t60'] = (df['sell_price'] / df['price_mean_t60']).astype(np.float16)\n",
    "    \n",
    "        \n",
    "    \n",
    "    # Adding mean/std target encoding features\n",
    "\n",
    "    # Columns for to encode\n",
    "#     icols =  [\n",
    "#             ['state_id'],\n",
    "#             ['store_id'],\n",
    "#             ['cat_id'],\n",
    "#             ['dept_id'],\n",
    "#             ['state_id', 'cat_id'],\n",
    "#             ['state_id', 'dept_id'],\n",
    "#             ['store_id', 'cat_id'],\n",
    "#             ['store_id', 'dept_id'],\n",
    "#             ['item_id'],\n",
    "#             ['item_id', 'state_id'],\n",
    "#             ['item_id', 'store_id']\n",
    "#             ]\n",
    "\n",
    "#     global target_enc_cols \n",
    "    \n",
    "#     for col in icols:\n",
    "#         print('Encoding', col)\n",
    "#         # TODO: Make this with variable, or may be use d column as integer\n",
    "#         temp_df = df[df['date'] < datetime(2016, 3, 28)] # to be sure we don't have leakage in our validation set\n",
    "\n",
    "#         temp_df = temp_df.groupby(col).agg({'sales': ['std','mean']})\n",
    "#         col_name = '_enc_'+'_'.join(col)+'_'\n",
    "#         new_columns = [col_name.join(col).strip() for col in temp_df.columns.values]\n",
    "#         temp_df.columns = new_columns\n",
    "#         temp_df = temp_df.reset_index()\n",
    "#         #print(temp_df)\n",
    "#         df = df.merge(temp_df, on=col, how='left')\n",
    "#         #print(df)\n",
    "#         # Save columns for later usage\n",
    "#         target_enc_cols += new_columns\n",
    "#         del temp_df\n",
    "#         gc.collect()\n",
    "    \n",
    "    date_features = {\n",
    "        \n",
    "        \"wday\": \"weekday\",\n",
    "        \"woy\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "#         \"ime\": \"is_month_end\",\n",
    "#         \"ims\": \"is_month_start\",\n",
    "    }\n",
    "    \n",
    "#     df.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n",
    "    \n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in df.columns:\n",
    "            df[date_feat_name] = df[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            df[date_feat_name] = getattr(df[\"date\"].dt, date_feat_func).astype(\"int16\")\n",
    "            \n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892426,
     "status": "ok",
     "timestamp": 1589284778382,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "6961UPSwScn-",
    "outputId": "bb34505c-372b-4b7f-85d1-1fc013dbbb3d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = create_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGln45lxScoF"
   },
   "outputs": [],
   "source": [
    "# Choose one day from data set, to have\n",
    "# all encodings for every id for later merge \n",
    "# with test data\n",
    "\n",
    "# mean_encodings_df = data.loc[data['d'] == 'd_1913', ['id'] + target_enc_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 901776,
     "status": "ok",
     "timestamp": 1589284787754,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "T3eyfph3ScoI",
    "outputId": "e5cf76cf-a657-4431-aa19-7a41f7741682",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 926139,
     "status": "ok",
     "timestamp": 1589284812130,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "fBeXEZnOScoL",
    "outputId": "edd8a756-5574-4aa0-c3d6-b89dee704e20"
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 926124,
     "status": "ok",
     "timestamp": 1589284812131,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "CFCx-ct1ScoO",
    "outputId": "eb71b70a-7aa5-4dbd-9ba0-e212d6255ad0"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bl9fovP1ScoR"
   },
   "source": [
    "## Reduce mem usage of created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 970776,
     "status": "ok",
     "timestamp": 1589284856810,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "QOlV3k5RScoR",
    "outputId": "9cb7908d-9136-42bc-edfa-c9bf0bf2cd92"
   },
   "outputs": [],
   "source": [
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 970757,
     "status": "ok",
     "timestamp": 1589284856811,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "UmHICxLLScoU",
    "outputId": "f642d348-de3a-4f17-b28d-e1e9f2ccac47"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOtQF4i-ScoX"
   },
   "source": [
    "# 3. Fit & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 970739,
     "status": "ok",
     "timestamp": 1589284856812,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "g09Nv5CvScoX",
    "outputId": "0acf8ed7-cdb7-43b2-ed45-1f8955911d97"
   },
   "outputs": [],
   "source": [
    "print('Data usage: {} GB'.format(data.memory_usage().sum() / 10**9))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byYTObXaScoa"
   },
   "outputs": [],
   "source": [
    "# train_end_dt = datetime(2016, 3, 27)\n",
    "# valid_end_dt = datetime(2016, 4, 24)\n",
    "\n",
    "valid_start = datetime(2016, 3, 28)\n",
    "train_valid_end_dt = datetime(2016, 4, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1004519,
     "status": "error",
     "timestamp": 1589284890643,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "b77yc0E7Scoc",
    "outputId": "b18b83c5-ee57-4cf3-fb75-5c7dbc4b1f06",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\", \"weights\"] + \\\n",
    "                ['lag_t1'] + ['dept_id_store_id_lag_t1', 'item_id_lag_t1'] # lag_t1 leads to overfitting\n",
    "train_cols = data.columns[~data.columns.isin(useless_cols)]\n",
    "#used_cols = train_cols.append(pd.Index(['weights'])) # with weights\n",
    "\n",
    "# Splitting train and validation by date (28 days before prediction horizon)\n",
    "# To drop na values only from training set\n",
    "# train = data.loc[data.date <= train_end_dt].dropna()\n",
    "X_train = data[train_cols]\n",
    "y_train = data[\"sales\"]\n",
    "\n",
    "X_valid= data.loc[(data.date >= valid_start) & (data.date <=train_valid_end_dt), train_cols]\n",
    "y_valid = data.loc[(data.date >= valid_start) & (data.date <= train_valid_end_dt), \"sales\"]\n",
    "\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "X_train_np = X_train.values.astype(np.float16)\n",
    "X_valid_np = X_valid.values.astype(np.float16)\n",
    "\n",
    "del X_train, X_valid\n",
    "gc.collect()\n",
    "\n",
    "train_data = lgb.Dataset(X_train_np, label = y_train, feature_name = list(train_cols), categorical_feature=cat_feats, free_raw_data=False)\n",
    "valid_data = lgb.Dataset(X_valid_np, label = y_valid, feature_name = list(train_cols), categorical_feature=cat_feats, free_raw_data=False)\n",
    "# train_data = lgb.Dataset(X_train[train_cols], label = y_train, weight=X_train['weights'], categorical_feature=cat_feats, free_raw_data=False)\n",
    "# valid_data = lgb.Dataset(X_valid[train_cols], label = y_valid, weight=X_valid['weights'], categorical_feature=cat_feats, free_raw_data=False)\n",
    "\n",
    "\n",
    "# Random train-validation split\n",
    "# X_train = data[used_cols]\n",
    "# y_train = data[\"sales\"]\n",
    "\n",
    "# np.random.seed(SEED)\n",
    "# valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
    "# train_inds = np.setdiff1d(X_train.index.values, valid_inds)\n",
    "\n",
    "# train_data = lgb.Dataset(X_train.loc[train_inds, train_cols] ,\\\n",
    "#                          label = y_train.loc[train_inds], weight=X_train.loc[train_inds, \"weights\"],\\\n",
    "#                          categorical_feature=cat_feats, free_raw_data=False)\n",
    "# valid_data = lgb.Dataset(X_train.loc[valid_inds, train_cols], \\\n",
    "#                          label = y_train.loc[valid_inds], weight=X_train.loc[valid_inds, \"weights\"],\\\n",
    "#                          categorical_feature=cat_feats, free_raw_data=False)\n",
    "# del valid_inds, train_inds\n",
    "\n",
    "# del X_train, X_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HZHWCE5Scof"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "#             'device' : 'gpu', # Need for local GPU computations\n",
    "#             'max_bin': 31, # For better GPU performance\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'tweedie',\n",
    "            'tweedie_variance_power': 1.1,\n",
    "            'metric': 'rmse',\n",
    "            'subsample': 0.5,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.03,\n",
    "            'num_leaves': 2**11-1,\n",
    "            'min_data_in_leaf': 2**12-1,\n",
    "            'feature_fraction': 0.5,\n",
    "            'max_bin': 100,\n",
    "            'n_estimators': 1400,\n",
    "            'boost_from_average': False,\n",
    "            'verbose': 1,\n",
    "            'n_jobs': 4, # For local computation optimization\n",
    "            'seed': SEED,\n",
    "} \n",
    "\n",
    "# params = {\n",
    "#         \"objective\" : \"poisson\",\n",
    "#         \"metric\" :\"rmse\",\n",
    "#         \"force_row_wise\" : True,\n",
    "#         \"learning_rate\" : 0.075,\n",
    "# #         \"sub_feature\" : 0.8,\n",
    "#         \"sub_row\" : 0.8,\n",
    "#         \"bagging_freq\" : 1,\n",
    "#         'feature_fraction': 0.8,\n",
    "#         \"lambda_l2\" : 0.1,\n",
    "# #         \"nthread\" : 4\n",
    "#         'verbosity': 1,\n",
    "#         'num_iterations' : 1200,\n",
    "#         'num_leaves': 2**7-1,\n",
    "#         \"min_data_in_leaf\": 2**7-1,\n",
    "#         'early_stopping_rounds': 125,\n",
    "#         'seed': SEED,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upLHPd6OSN3E"
   },
   "outputs": [],
   "source": [
    "# # Trick to reduce memory spike while model starts training\n",
    "# train_data.save_binary(f'{DATA_OUTPUT_DIR}/train.bin')\n",
    "# valid_data.save_binary(f'{DATA_OUTPUT_DIR}/valid.bin')\n",
    "# del train_data, valid_data\n",
    "# gc.collect()\n",
    "# train_data = lgb.Dataset(f'{DATA_OUTPUT_DIR}/train.bin', categorical_feature=cat_feats, two_round=True)\n",
    "# valid_data = lgb.Dataset(f'{DATA_OUTPUT_DIR}/valid.bin', categorical_feature=cat_feats, two_round=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5553239,
     "status": "ok",
     "timestamp": 1589290457436,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "yK5AypcDScoh",
    "outputId": "71d02d96-197b-481b-c6b3-facc4dddfed0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "m_lgb = lgb.train(params, train_data, valid_sets = [train_data, valid_data], \n",
    "                  verbose_eval=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5552697,
     "status": "ok",
     "timestamp": 1589290457680,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "O68Xo86bScok",
    "outputId": "f1c85204-4352-42e1-8ed3-c605c68a3a56"
   },
   "outputs": [],
   "source": [
    "os.system('say \"Training complete\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUyVtgQVScom"
   },
   "outputs": [],
   "source": [
    "m_lgb.save_model(f'{DATA_OUTPUT_DIR}/model.lgb')\n",
    "m_lgb = lgb.Booster(model_file=f'{DATA_OUTPUT_DIR}/model.lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5556789,
     "status": "ok",
     "timestamp": 1589290462444,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "0Tu0YHcMScop",
    "outputId": "4cbe0f1f-e951-4e3a-f74b-25be6aa70e66"
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\"Value\": m_lgb.feature_importance(\"gain\"), \"Feature\": m_lgb.feature_name()}) \\\n",
    "                    .sort_values(by=\"Value\", ascending=False)\n",
    "\n",
    "# Change size of the plot, so we can see all features\n",
    "fig_dims = (10, 14)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", ax=ax, data=feature_importance)\n",
    "plt.title('LightGBM Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5556576,
     "status": "ok",
     "timestamp": 1589290462444,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "FkxzN0c2Scot",
    "outputId": "8a387adb-4ecd-4fd2-ab3d-53a0ea2ccead"
   },
   "outputs": [],
   "source": [
    "# Detection of features with zero-importance\n",
    "zero_features = list(feature_importance[feature_importance['Value'] == 0]['Feature'])\n",
    "print('\\nThere are {} features with 0.0 importance'.format(len(zero_features)))\n",
    "print(zero_features)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5559395,
     "status": "ok",
     "timestamp": 1589290465454,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "HQ_SkePyScov",
    "outputId": "4d4c82d5-e7d4-4445-940b-7535dc96adeb"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "tdata = pd.read_pickle(f'{DATA_GRID_INPUT_DIR}/m5_data_test_model2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_5XcWcyScoy"
   },
   "outputs": [],
   "source": [
    "def create_lag_features_for_test(df, day):\n",
    "    \n",
    "#     agg_levels = [['dept_id', 'store_id'],\n",
    "#                   ['item_id']\n",
    "#                   ]\n",
    "#     agg_level_names = ['_'.join(level) for level in agg_levels]\n",
    "    \n",
    "#     # Create dataframes grouped by agg_levels and date\n",
    "#     agg_df = dict()\n",
    "#     for level, level_name in zip(agg_levels, agg_level_names):\n",
    "#         agg_df[level_name] = df[['id', 'date', 'item_id', 'dept_id', 'store_id', 'sales']].groupby(level + ['date'])['sales'].sum().reset_index()\n",
    "      \n",
    "    # create lag feaures just for single day (faster)\n",
    "    lags = [1, 7, 14, 28]\n",
    "    lag_cols = [f\"lag_t{lag}\" for lag in lags]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        df.loc[df.date == day, lag_col] = df.loc[df.date ==day-timedelta(days=lag), 'sales'].values  # !!! main\n",
    "        \n",
    "#         # Setting lags for agg levels\n",
    "#         if lag in [1, 7, 28]:\n",
    "#             for level, level_name in zip(agg_levels, agg_level_names):\n",
    "#                 agg_df[level_name][level_name + '_' + lag_col] = agg_df[level_name].groupby(level)['sales'].shift(lag)\n",
    "\n",
    "    wins = [7, 14, 28, 60]\n",
    "    for win in wins:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            df_win = df[(df.date <= day-timedelta(days=lag)) & (df.date > day-timedelta(days=lag+win))]\n",
    "            df_win_grouped_mean = df_win.groupby(\"id\").agg({'sales':'mean'}).reindex(df.loc[df.date==day,'id'])\n",
    "            df.loc[df.date == day,f\"rmean_{lag}_{win}\"] = df_win_grouped_mean.sales.values            \n",
    "\n",
    "            df_win_grouped_median = df_win.groupby(\"id\").agg({'sales':'median'}).reindex(df.loc[df.date==day,'id'])\n",
    "            df.loc[df.date == day,f\"rmedian_{lag}_{win}\"] = df_win_grouped_median.sales.values\n",
    "            df_win_grouped_std = df_win.groupby(\"id\").agg({'sales':'std'}).reindex(df.loc[df.date==day,'id'])\n",
    "            df.loc[df.date == day,f\"rstd_{lag}_{win}\"] = df_win_grouped_std.sales.values\n",
    "\n",
    "            df[f'rmean_{lag}_{win}_decay'] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x: x.ewm(span=win).mean()).astype(np.float16)\n",
    "#             df[f\"rmedian_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).median()).astype(np.float16)\n",
    "#             df[f\"rstd_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).std()).astype(np.float16)\n",
    "\n",
    "#             # Computing rollings for aggregation levels\n",
    "#             if (lag in [1, 7, 28]) and (win in [7, 28]):\n",
    "#                 for level, level_name in zip(agg_levels, agg_level_names):\n",
    "#                     agg_df[level_name][level_name + '_' + f'rmean_{lag}_{win}'] = agg_df[level_name].groupby(level)[level_name + '_' + lag_col].transform(lambda x: x.rolling(win).mean())\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Merging agg levels computations with main grid\n",
    "    for level, level_name in zip(agg_levels, agg_level_names):\n",
    "        agg_df[level_name].drop(['sales'], axis=1, inplace=True)\n",
    "        df = df.merge(agg_df[level_name], on=level + ['date'], copy=False)    \n",
    "\n",
    "    return df\n",
    "    \n",
    "    \n",
    "## Creating features for test data\n",
    "def create_static_features_for_test(df):\n",
    "    # We create lags here, so we can use them later \n",
    "    # for weighted moving average computations\n",
    "    lags = [1, 7, 14, 28]\n",
    "    lag_cols = [f\"lag_t{lag}\" for lag in lags ]\n",
    "\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        df[lag_col] = df[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag).astype(np.float16)\n",
    "    \n",
    "    # copy of the code from `create_df()` above\n",
    "    date_features = {\n",
    "        \"wday\": \"weekday\",\n",
    "        \"woy\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "    }\n",
    "\n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in df.columns:\n",
    "            df[date_feat_name] = df[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            df[date_feat_name] = getattr(\n",
    "                df[\"date\"].dt, date_feat_func).astype(\"int16\")\n",
    "            \n",
    "    # Create price features\n",
    "    df['price_mean_t60'] = df[['id','sell_price']].groupby([\"id\"])[\"sell_price\"].transform(lambda x: x.rolling(60).mean()).astype(np.float16)\n",
    "    df['price_momentum_t60'] = (df['sell_price'] / df['price_mean_t60']).astype(np.float16)\n",
    "    \n",
    "    # Add mean encoding features\n",
    "#     global mean_encodings_df\n",
    "#     df = df.merge(mean_encodings_df, on=['id'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQhwlXwWSco0"
   },
   "outputs": [],
   "source": [
    "tdata = create_static_features_for_test(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYkqn_tXSco2"
   },
   "outputs": [],
   "source": [
    "# # FOR TEST\n",
    "# day = FIRST_PRED_DAY + timedelta(days=0)\n",
    "# print(i, day)\n",
    "# tst = tdata[(tdata.date >= day - timedelta(days=max_lags)) & (tdata.date <= day)].copy()\n",
    "# create_lag_features_for_test(tst, day)\n",
    "# tst = tst.loc[tst.date == day, train_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wWp1yUESco5"
   },
   "outputs": [],
   "source": [
    "# os.system(\"say 'Task complete'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsLDlygzSco7"
   },
   "outputs": [],
   "source": [
    "# tst[tst.isna().any(axis=1)].shape[0] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 336990,
     "status": "ok",
     "timestamp": 1589196544586,
     "user": {
      "displayName": "Alan Kabisov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64",
      "userId": "16237523698272594904"
     },
     "user_tz": -180
    },
    "id": "8-fiqaKgSco-",
    "outputId": "7335de4b-ba89-4399-9393-33cb2fba24ce",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(0, 28):\n",
    "    day = FIRST_PRED_DAY + timedelta(days=i)\n",
    "    print(i, day)\n",
    "    tst = tdata[(tdata.date >= day - timedelta(days=max_lags)) & (tdata.date <= day)].copy()\n",
    "    tst = create_lag_features_for_test(tst, day)\n",
    "    tst = tst.loc[tst.date == day, train_cols]\n",
    "    # Check that all features generated correctly\n",
    "    if tst[tst.isna().any(axis=1)].shape[0] > 0:\n",
    "        print('Some values in tst are nans:')\n",
    "        print(tst[tst.isna().any(axis=1)])\n",
    "    tdata.loc[tdata.date == day, \"sales\"] = m_lgb.predict(tst.values.astype(np.float32)) # 1.035*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWPcB8PsScpB"
   },
   "outputs": [],
   "source": [
    "os.system('say \"Prediction complete\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aY8PmJRrScpD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tdata.loc[(tdata.date >= FIRST_PRED_DAY) & (tdata.sales > 2)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "It2DqbEKScpG"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tdata_sub = tdata.loc[tdata.date >= FIRST_PRED_DAY, [\"id\", \"sales\"]].copy()\n",
    "tdata_sub.loc[tdata.date >= FIRST_PRED_DAY+ timedelta(days=h), \"id\"] = tdata_sub.loc[tdata.date >= FIRST_PRED_DAY+timedelta(days=h), \n",
    "                                                                     \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "tdata_sub[\"F\"] = [f\"F{rank}\" for rank in tdata_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "tdata_sub = tdata_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][[f\"F{i}\" for i in range(1,29)]].reset_index()\n",
    "tdata_sub.fillna(0., inplace = True)\n",
    "\n",
    "# # kyakovlev magic trick\n",
    "# for i in range(1,29):\n",
    "#     tdata_sub['F'+str(i)] *= 1.02\n",
    "\n",
    "tdata_sub.to_csv(f\"{DATA_OUTPUT_DIR}/submission.csv\",index=False)\n",
    "tdata_sub.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PiRuVW5LScpI"
   },
   "outputs": [],
   "source": [
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwhFNtfmScpM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "AlGreKo - Baseline Model (LGBM Day By Day).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Data Science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
