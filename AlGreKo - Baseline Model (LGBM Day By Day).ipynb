{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"AlGreKo - Baseline Model (LGBM Day By Day).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4ILV68NGScnf","colab_type":"text"},"source":["# 1. Loading Data"]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"HKH9EkudScng","colab_type":"code","outputId":"13951bd4-fbdf-4068-f0ec-30255960a81d","executionInfo":{"status":"ok","timestamp":1588871502306,"user_tz":-180,"elapsed":1061,"user":{"displayName":"Alan Kabisov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64","userId":"16237523698272594904"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["import sys\n","import gc\n","import os\n","import warnings\n","import pickle\n","import statsmodels.api as sm\n","from pylab import rcParams\n","import time\n","from  datetime import datetime, timedelta\n","\n","import pandas as pd\n","from pandas.plotting import register_matplotlib_converters\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import lightgbm as lgb\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn import preprocessing, metrics\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","pd.set_option(\"display.max_columns\", 500)\n","pd.set_option(\"display.max_rows\", 500)\n","\n","register_matplotlib_converters()\n","sns.set()\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"WjlAJnT2Scnm","colab_type":"text"},"source":["## 1.1 Functions"]},{"cell_type":"code","metadata":{"id":"9AbC-zQIScnm","colab_type":"code","colab":{}},"source":["def reduce_mem_usage(df, verbose=True):\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2    \n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)    \n","    end_mem = df.memory_usage().sum() / 1024**2\n","    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n","    return df\n","\n","\n","def display_missing(df):    \n","    for col in df.columns.tolist():  \n","        if df[col].isnull().sum() != 0:\n","            print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n","    print('\\n')\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ti6p45VScnq","colab_type":"text"},"source":["## 1.2 Loading data grid"]},{"cell_type":"code","metadata":{"id":"RwWG8OiRpf_l","colab_type":"code","outputId":"7c3caed5-6e20-4db4-feed-38ab5fb421ea","executionInfo":{"status":"ok","timestamp":1588871502542,"user_tz":-180,"elapsed":1270,"user":{"displayName":"Alan Kabisov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64","userId":"16237523698272594904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Mount google drive\n","if IN_COLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jNE6Zy8zScnq","colab_type":"code","outputId":"629f0425-de89-430c-87ca-fa18524ea86f","executionInfo":{"status":"ok","timestamp":1588871504090,"user_tz":-180,"elapsed":2800,"user":{"displayName":"Alan Kabisov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64","userId":"16237523698272594904"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Setting directories where data is stored and ouptut dir\n","if IN_COLAB:\n","  DATA_GRID_INPUT_DIR = './drive/My Drive/Colab Notebooks' \n","  DATA_OUTPUT_DIR = './drive/My Drive/Colab Notebooks'\n","  !ls './drive/My Drive/Colab Notebooks'\n","else:\n","  DATA_GRID_INPUT_DIR = 'data'\n","  DATA_OUTPUT_DIR = '.'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["'AlGreKo - Baseline Model (LGBM Day By Day).ipynb'   model.lgb\n"," m5_data_model2.pkl\t\t\t\t     submission.csv\n"," m5_data_test_model2.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UtLS2plBScnt","colab_type":"code","outputId":"8ea86c4a-3b5a-4f7e-9939-1219b6d9c079","executionInfo":{"status":"ok","timestamp":1588871545129,"user_tz":-180,"elapsed":43827,"user":{"displayName":"Alan Kabisov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggaoq3IzIT_ByiqQ58t3SgQ7XFzogwq6EUCAhHSlrE=s64","userId":"16237523698272594904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print('Loading the data...')\n","\n","data = pd.read_pickle(f'{DATA_GRID_INPUT_DIR}/m5_data_model2.pkl')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Loading the data...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aNYreOEtScnw","colab_type":"text"},"source":["## 1.3 Init variables"]},{"cell_type":"code","metadata":{"id":"0iuKum57Scnw","colab_type":"code","colab":{}},"source":["h = 28 # Prediction horizon\n","max_lags = 120 # Max lags used\n","TRAINING_LAST_DAY_NUM = 1913 # Last day for training data\n","FIRST_PRED_DAY = datetime(2016,4, 25) # First prediction day\n","FIRST_LOADING_DAY = datetime(2013, 4,7) # First day for training\n","FIRST_LOADING_DAY_NUM = 800\n","SEED = 7\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3_SRy3RScn3","colab_type":"text"},"source":["# 2. Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"cQIkNyyEScn4","colab_type":"text"},"source":["## Creating features\n"]},{"cell_type":"code","metadata":{"id":"4FZXdWPSv9hd","colab_type":"code","colab":{}},"source":["# data = data.loc[data.date > '2015-01-01'] "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BeAoryHtwFBS","colab_type":"code","colab":{}},"source":["# data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yjdkw71jv0b2","colab_type":"code","colab":{}},"source":["# dept_store_df = data[['id', 'date', 'item_id', 'dept_id', 'store_id', 'sales']].groupby(['dept_id', 'store_id', 'date'])['sales'].sum().reset_index()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_jVVBq0wEdw","colab_type":"code","colab":{}},"source":["# dept_store_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"23U28rm2shvO","colab_type":"code","colab":{}},"source":["# dept_store_df['dept_store_lag_3'] = dept_store_df.groupby(['dept_id', 'store_id'])['sales'].shift(28)\n","# dept_store_df['dept_sotre_rmean_3_3'] = dept_store_df.groupby(['dept_id', 'store_id'])['dept_store_lag_3'].transform(lambda x: x.rolling(3).mean())\n","# dept_store_df.drop(['sales'], axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ziP0x2bQ1mwU","colab_type":"code","colab":{}},"source":["# dept_store_df.loc[dept_store_df.date=='2016-04-23']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1PEVYdY2EnB","colab_type":"code","colab":{}},"source":["# data = data.merge(dept_store_df, on=['dept_id', 'store_id', 'date'], copy=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"866wfXsB3WzP","colab_type":"code","colab":{}},"source":["# data.loc[data.date=='2016-04-23']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTWdgH6ZScn4","colab_type":"code","colab":{}},"source":["# Init global variable to store columns of encodings\n","# To use them later for test dataset\n","\n","# target_enc_cols = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lESYikvOScn7","colab_type":"code","colab":{}},"source":["def create_features(df):\n","\n","    # agg_levels = [['dept_id', 'store_id'],\n","    #               ['item_id']\n","    #               ]\n","    # agg_level_names = ['_'.join(level) for level in agg_levels]\n","    \n","    # # Create dataframes grouped by agg_levels and date\n","    # agg_df = dict()\n","    # for level, level_name in zip(agg_levels, agg_level_names):\n","    #   agg_df[level_name] = df[['id', 'date', 'item_id', 'dept_id', 'store_id', 'sales']].groupby(level + ['date'])['sales'].sum().reset_index()\n","\n","    lags = [7, 28]\n","    lag_cols = [f\"lag_t{lag}\" for lag in lags ]\n","  \n","    for lag, lag_col in zip(lags, lag_cols):\n","        df[lag_col] = df[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag).astype(np.float16)\n","        \n","        # Setting lags for agg levels\n","        # for level, level_name in zip(agg_levels, agg_level_names):\n","        #   agg_df[level_name][level_name + '_' + lag_col] = agg_df[level_name].groupby(level)['sales'].shift(lag)\n","\n","    wins = [7, 28]\n","    for win in wins:\n","        for lag, lag_col in zip(lags, lag_cols):\n","            df[f\"rmean_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean()).astype(np.float16)\n","            \n","            # Computing rollings for aggregation levels\n","            # for level, level_name in zip(agg_levels, agg_level_names):\n","            #   agg_df[level_name][level_name + '_' + f'rmean_{lag}_{win}'] = agg_df[level_name].groupby(level)[level_name + '_' + lag_col].transform(lambda x: x.rolling(win).mean())\n","            \n","            \n","            #df[f\"rmedian_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).median()).astype(np.float16)\n","            #df[f\"rdiff_mean_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).diff().mean()).astype(np.float16)\n","            #df[f\"rstd_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).std()).astype(np.float16)\n","            #df[f'rmean_{lag}_{win}_decay'] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x: x.ewm(span=win).mean()).astype(np.float16)\n","\n","    # Merging agg levels computations with main grid\n","    # for level, level_name in zip(agg_levels, agg_level_names):\n","    #     agg_df[level_name].drop(['sales'], axis=1, inplace=True)\n","    #     df = df.merge(agg_df[level_name], on=level + ['date'], copy=False)\n","\n","    df['price_mean_t60'] = df[['id','sell_price']].groupby([\"id\"])[\"sell_price\"].transform(lambda x: x.rolling(60).mean()).astype(np.float16)\n","    df['price_momentum_t60'] = (df['sell_price'] / df['price_mean_t60']).astype(np.float16)\n","    \n","        \n","    \n","    # Adding mean/std target encoding features\n","\n","    # Columns for to encode\n","#     icols =  [\n","#             ['state_id'],\n","#             ['store_id'],\n","#             ['cat_id'],\n","#             ['dept_id'],\n","#             ['state_id', 'cat_id'],\n","#             ['state_id', 'dept_id'],\n","#             ['store_id', 'cat_id'],\n","#             ['store_id', 'dept_id'],\n","#             ['item_id'],\n","#             ['item_id', 'state_id'],\n","#             ['item_id', 'store_id']\n","#             ]\n","\n","#     global target_enc_cols \n","    \n","#     for col in icols:\n","#         print('Encoding', col)\n","#         # TODO: Make this with variable, or may be use d column as integer\n","#         temp_df = df[df['date'] < datetime(2016, 3, 28)] # to be sure we don't have leakage in our validation set\n","\n","#         temp_df = temp_df.groupby(col).agg({'sales': ['std','mean']})\n","#         col_name = '_enc_'+'_'.join(col)+'_'\n","#         new_columns = [col_name.join(col).strip() for col in temp_df.columns.values]\n","#         temp_df.columns = new_columns\n","#         temp_df = temp_df.reset_index()\n","#         #print(temp_df)\n","#         df = df.merge(temp_df, on=col, how='left')\n","#         #print(df)\n","#         # Save columns for later usage\n","#         target_enc_cols += new_columns\n","#         del temp_df\n","#         gc.collect()\n","    \n","    date_features = {\n","        \n","        \"wday\": \"weekday\",\n","        \"woy\": \"weekofyear\",\n","        \"month\": \"month\",\n","        \"quarter\": \"quarter\",\n","        \"year\": \"year\",\n","        \"mday\": \"day\",\n","#         \"ime\": \"is_month_end\",\n","#         \"ims\": \"is_month_start\",\n","    }\n","    \n","#     df.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n","    \n","    for date_feat_name, date_feat_func in date_features.items():\n","        if date_feat_name in df.columns:\n","            df[date_feat_name] = df[date_feat_name].astype(\"int16\")\n","        else:\n","            df[date_feat_name] = getattr(df[\"date\"].dt, date_feat_func).astype(\"int16\")\n","            \n","    \n","    return df\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6961UPSwScn-","colab_type":"code","colab":{}},"source":["%%time\n","\n","data = create_features(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGln45lxScoF","colab_type":"code","colab":{}},"source":["# Choose one day from data set, to have\n","# all encodings for every id for later merge \n","# with test data\n","\n","# mean_encodings_df = data.loc[data['d'] == 'd_1913', ['id'] + target_enc_cols].copy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"T3eyfph3ScoI","colab_type":"code","colab":{}},"source":["data.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBeXEZnOScoL","colab_type":"code","colab":{}},"source":["data.dropna(inplace = True)\n","data.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CFCx-ct1ScoO","colab_type":"code","colab":{}},"source":["data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bl9fovP1ScoR","colab_type":"text"},"source":["## Reduce mem usage of created features"]},{"cell_type":"code","metadata":{"id":"QOlV3k5RScoR","colab_type":"code","colab":{}},"source":["data = reduce_mem_usage(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmHICxLLScoU","colab_type":"code","colab":{}},"source":["gc.collect()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TOtQF4i-ScoX","colab_type":"text"},"source":["# 3. Fit & Predict"]},{"cell_type":"code","metadata":{"id":"g09Nv5CvScoX","colab_type":"code","colab":{}},"source":["print('Data usage: {} GB'.format(data.memory_usage().sum() / 10**9))\n","data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"byYTObXaScoa","colab_type":"code","colab":{}},"source":["# train_end_dt = datetime(2016, 3, 27)\n","# valid_end_dt = datetime(2016, 4, 24)\n","\n","valid_start = datetime(2016, 3, 28)\n","train_valid_end_dt = datetime(2016, 4, 27)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJhiLk2BaWaw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"b77yc0E7Scoc","colab_type":"code","colab":{}},"source":["%%time\n","\n","cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n","useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\", \"weights\"]\n","train_cols = data.columns[~data.columns.isin(useless_cols)]\n","#used_cols = train_cols.append(pd.Index(['weights'])) # with weights\n","\n","# Splitting train and validation by date (28 days before prediction horizon)\n","# To drop na values only from training set\n","# train = data.loc[data.date <= train_end_dt].dropna()\n","X_train = data[train_cols]\n","y_train = data[\"sales\"]\n","\n","X_valid= data.loc[(data.date >= valid_start) & (data.date <=tr ain_valid_end_dt), train_cols]\n","y_valid = data.loc[(data.date >= valid_start) & (data.date <= train_valid_end_dt), \"sales\"]\n","\n","X_train_np = X_train.values.astype(np.float32)\n","X_valid_np = X_valid.values.astype(np.float32)\n","\n","del X_train, X_valid\n","gc.collect()\n","\n","train_data = lgb.Dataset(X_train_np, label = y_train, feature_name = list(train_cols), categorical_feature=cat_feats, free_raw_data=False)\n","valid_data = lgb.Dataset(X_valid_np, label = y_valid, feature_name = list(train_cols), categorical_feature=cat_feats, free_raw_data=False)\n","# train_data = lgb.Dataset(X_train[train_cols], label = y_train, weight=X_train['weights'], categorical_feature=cat_feats, free_raw_data=False)\n","# valid_data = lgb.Dataset(X_valid[train_cols], label = y_valid, weight=X_valid['weights'], categorical_feature=cat_feats, free_raw_data=False)\n","\n","\n","# Random train-validation split\n","# X_train = data[used_cols]\n","# y_train = data[\"sales\"]\n","\n","# np.random.seed(SEED)\n","# valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n","# train_inds = np.setdiff1d(X_train.index.values, valid_inds)\n","\n","# train_data = lgb.Dataset(X_train.loc[train_inds, train_cols] ,\\\n","#                          label = y_train.loc[train_inds], weight=X_train.loc[train_inds, \"weights\"],\\\n","#                          categorical_feature=cat_feats, free_raw_data=False)\n","# valid_data = lgb.Dataset(X_train.loc[valid_inds, train_cols], \\\n","#                          label = y_train.loc[valid_inds], weight=X_train.loc[valid_inds, \"weights\"],\\\n","#                          categorical_feature=cat_feats, free_raw_data=False)\n","# del valid_inds, train_inds\n","\n","del data \n","\n","gc.collect()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HZHWCE5Scof","colab_type":"code","colab":{}},"source":["params = {\n","            'boosting_type': 'gbdt',\n","            'objective': 'tweedie',\n","            'tweedie_variance_power': 1.1,\n","            'metric': 'rmse',\n","            'subsample': 0.5,\n","            'subsample_freq': 1,\n","            'learning_rate': 0.03,\n","            'num_leaves': 2**11-1,\n","            'min_data_in_leaf': 2**12-1,\n","            'feature_fraction': 0.5,\n","            'max_bin': 100,\n","            'n_estimators': 1400,\n","            'boost_from_average': False,\n","            'verbose': 1,\n","            'seed': SEED,\n","} \n","\n","# params = {\n","#         \"objective\" : \"poisson\",\n","#         \"metric\" :\"rmse\",\n","#         \"force_row_wise\" : True,\n","#         \"learning_rate\" : 0.075,\n","# #         \"sub_feature\" : 0.8,\n","#         \"sub_row\" : 0.8,\n","#         \"bagging_freq\" : 1,\n","#         'feature_fraction': 0.8,\n","#         \"lambda_l2\" : 0.1,\n","# #         \"nthread\" : 4\n","#         'verbosity': 1,\n","#         'num_iterations' : 1200,\n","#         'num_leaves': 2**7-1,\n","#         \"min_data_in_leaf\": 2**7-1,\n","#         'early_stopping_rounds': 125,\n","#         'seed': SEED,\n","# }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"upLHPd6OSN3E","colab_type":"code","colab":{}},"source":["# # Trick to reduce memory spike while model starts training\n","# train_data.save_binary(f'{DATA_OUTPUT_DIR}/train.bin')\n","# valid_data.save_binary(f'{DATA_OUTPUT_DIR}/valid.bin')\n","# del train_data, valid_data\n","# gc.collect()\n","# train_data = lgb.Dataset(f'{DATA_OUTPUT_DIR}/train.bin', categorical_feature=cat_feats, two_round=True)\n","# valid_data = lgb.Dataset(f'{DATA_OUTPUT_DIR}/valid.bin', categorical_feature=cat_feats, two_round=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yK5AypcDScoh","colab_type":"code","colab":{}},"source":["%%time\n","\n","m_lgb = lgb.train(params, train_data, valid_sets = [train_data, valid_data], \n","                  verbose_eval=50)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O68Xo86bScok","colab_type":"code","colab":{}},"source":["os.system('say \"Training complete\"')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUyVtgQVScom","colab_type":"code","colab":{}},"source":["m_lgb.save_model(f'{DATA_OUTPUT_DIR}/model.lgb')\n","#m_lgb = lgb.Booster(model_file=f'{DATA_OUTPUT_DIR}/model.lgb')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Tu0YHcMScop","colab_type":"code","colab":{}},"source":["feature_importance = pd.DataFrame({\"Value\": m_lgb.feature_importance(\"gain\"), \"Feature\": m_lgb.feature_name()}) \\\n","                    .sort_values(by=\"Value\", ascending=False)\n","\n","# Change size of the plot, so we can see all features\n","fig_dims = (10, 14)\n","fig, ax = plt.subplots(figsize=fig_dims)\n","\n","sns.barplot(x=\"Value\", y=\"Feature\", ax=ax, data=feature_importance)\n","plt.title('LightGBM Features')\n","plt.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FkxzN0c2Scot","colab_type":"code","colab":{}},"source":["# Detection of features with zero-importance\n","zero_features = list(feature_importance[feature_importance['Value'] == 0]['Feature'])\n","print('\\nThere are {} features with 0.0 importance'.format(len(zero_features)))\n","print(zero_features)\n","feature_importance"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQ_SkePyScov","colab_type":"code","colab":{}},"source":["%%time \n","\n","tdata = pd.read_pickle(f'{DATA_GRID_INPUT_DIR}/m5_data_test_model2.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_5XcWcyScoy","colab_type":"code","colab":{}},"source":["def create_lag_features_for_test(df, day):\n","    \n","    # agg_levels = [['dept_id', 'store_id'],\n","    #               ['item_id']\n","    #               ]\n","    # agg_level_names = ['_'.join(level) for level in agg_levels]\n","    \n","    # # Create dataframes grouped by agg_levels and date\n","    # agg_df = dict()\n","    # for level, level_name in zip(agg_levels, agg_level_names):\n","    #   agg_df[level_name] = df[['id', 'date', 'item_id', 'dept_id', 'store_id', 'sales']].groupby(level + ['date'])['sales'].sum().reset_index()\n","      \n","    # create lag feaures just for single day (faster)\n","    lags = [7, 28]\n","    lag_cols = [f\"lag_t{lag}\" for lag in lags]\n","    for lag, lag_col in zip(lags, lag_cols):\n","      df.loc[df.date == day, lag_col] = df.loc[df.date ==day-timedelta(days=lag), 'sales'].values  # !!! main\n","      \n","    #   # Setting lags for agg levels\n","    #   for level, level_name in zip(agg_levels, agg_level_names):\n","    #       agg_df[level_name][level_name + '_' + lag_col] = agg_df[level_name].groupby(level)['sales'].shift(lag)\n","\n","    wins = [7, 28]\n","    for win in wins:\n","        for lag, lag_col in zip(lags, lag_cols):\n","            df_win = df[(df.date <= day-timedelta(days=lag)) & (df.date > day-timedelta(days=lag+win))]\n","            df_win_grouped_mean = df_win.groupby(\"id\").agg({'sales':'mean'}).reindex(df.loc[df.date==day,'id'])\n","            df.loc[df.date == day,f\"rmean_{lag}_{win}\"] = df_win_grouped_mean.sales.values\n","\n","            #  # Computing rollings for aggregation levels\n","            # for level, level_name in zip(agg_levels, agg_level_names):\n","            #   agg_df[level_name][level_name + '_' + f'rmean_{lag}_{win}'] = agg_df[level_name].groupby(level)[level_name + '_' + lag_col].transform(lambda x: x.rolling(win).mean())\n","            \n","            \n","\n","            # df_win_grouped_median = df_win.groupby(\"id\").agg({'sales':'median'}).reindex(df.loc[df.date==day,'id'])\n","            # df.loc[df.date == day,f\"rmedian_{lag}_{win}\"] = df_win_grouped_median.sales.values\n","            # df_win_grouped_std = df_win.groupby(\"id\").agg({'sales':'std'}).reindex(df.loc[df.date==day,'id'])\n","            # df.loc[df.date == day,f\"rstd_{lag}_{win}\"] = df_win_grouped_std.sales.values\n","\n","            # df[f'rmean_{lag}_{win}_decay'] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x: x.ewm(span=win).mean()).astype(np.float16)\n","#             df[f\"rmedian_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).median()).astype(np.float16)\n","#             df[f\"rstd_{lag}_{win}\"] = df[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).std()).astype(np.float16)\n","    \n","    \n","    # Merging agg levels computations with main grid\n","    # for level, level_name in zip(agg_levels, agg_level_names):\n","    #     agg_df[level_name].drop(['sales'], axis=1, inplace=True)\n","    #     df = df.merge(agg_df[level_name], on=level + ['date'], copy=False)    \n","\n","    return df\n","    \n","    \n","## Creating features for test data\n","def create_static_features_for_test(df):\n","    # We create lags here, so we can use them later \n","    # for weighted moving average computations\n","    lags = [7, 28, 56]\n","    lag_cols = [f\"lag_t{lag}\" for lag in lags ]\n","\n","    for lag, lag_col in zip(lags, lag_cols):\n","        df[lag_col] = df[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag).astype(np.float16)\n","    \n","    # copy of the code from `create_df()` above\n","    date_features = {\n","        \"wday\": \"weekday\",\n","        \"woy\": \"weekofyear\",\n","        \"month\": \"month\",\n","        \"quarter\": \"quarter\",\n","        \"year\": \"year\",\n","        \"mday\": \"day\",\n","    }\n","\n","    for date_feat_name, date_feat_func in date_features.items():\n","        if date_feat_name in df.columns:\n","            df[date_feat_name] = df[date_feat_name].astype(\"int16\")\n","        else:\n","            df[date_feat_name] = getattr(\n","                df[\"date\"].dt, date_feat_func).astype(\"int16\")\n","            \n","    # Create price features\n","    df['price_mean_t60'] = df[['id','sell_price']].groupby([\"id\"])[\"sell_price\"].transform(lambda x: x.rolling(60).mean()).astype(np.float16)\n","    df['price_momentum_t60'] = (df['sell_price'] / df['price_mean_t60']).astype(np.float16)\n","    \n","    # Add mean encoding features\n","#     global mean_encodings_df\n","#     df = df.merge(mean_encodings_df, on=['id'])\n","\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQhwlXwWSco0","colab_type":"code","colab":{}},"source":["tdata = create_static_features_for_test(tdata)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYkqn_tXSco2","colab_type":"code","colab":{}},"source":["# # FOR TEST\n","# day = FIRST_PRED_DAY + timedelta(days=0)\n","# print(i, day)\n","# tst = tdata[(tdata.date >= day - timedelta(days=max_lags)) & (tdata.date <= day)].copy()\n","# create_lag_features_for_test(tst, day)\n","# tst = tst.loc[tst.date == day, train_cols]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wWp1yUESco5","colab_type":"code","colab":{}},"source":["# os.system(\"say 'Task complete'\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tsLDlygzSco7","colab_type":"code","colab":{}},"source":["# tst[tst.isna().any(axis=1)].shape[0] > 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"8-fiqaKgSco-","colab_type":"code","colab":{}},"source":["%%time\n","\n","for i in range(0, 28):\n","    day = FIRST_PRED_DAY + timedelta(days=i)\n","    print(i, day)\n","    tst = tdata[(tdata.date >= day - timedelta(days=max_lags)) & (tdata.date <= day)].copy()\n","    tst = create_lag_features_for_test(tst, day)\n","    tst = tst.loc[tst.date == day, train_cols]\n","    # Check that all features generated correctly\n","    if tst[tst.isna().any(axis=1)].shape[0] > 0:\n","        print('Some values in tst are nans:')\n","        print(tst[tst.isna().any(axis=1)])\n","    tdata.loc[tdata.date == day, \"sales\"] = 1.03*m_lgb.predict(tst.values.astype(np.float32)) \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWPcB8PsScpB","colab_type":"code","colab":{}},"source":["os.system('say \"Prediction complete\"')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"aY8PmJRrScpD","colab_type":"code","colab":{}},"source":["tdata.loc[(tdata.date >= FIRST_PRED_DAY) & (tdata.sales > 2)].count()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"It2DqbEKScpG","colab_type":"code","colab":{}},"source":["%%time\n","\n","tdata_sub = tdata.loc[tdata.date >= FIRST_PRED_DAY, [\"id\", \"sales\"]].copy()\n","tdata_sub.loc[tdata.date >= FIRST_PRED_DAY+ timedelta(days=h), \"id\"] = tdata_sub.loc[tdata.date >= FIRST_PRED_DAY+timedelta(days=h), \n","                                                                     \"id\"].str.replace(\"validation$\", \"evaluation\")\n","tdata_sub[\"F\"] = [f\"F{rank}\" for rank in tdata_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n","tdata_sub = tdata_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][[f\"F{i}\" for i in range(1,29)]].reset_index()\n","tdata_sub.fillna(0., inplace = True)\n","\n","# kyakovlev magic trick\n","# for i in range(1,29):\n","#     tdata_sub['F'+str(i)] *= 1.03\n","\n","tdata_sub.to_csv(f\"{DATA_OUTPUT_DIR}/submission.csv\",index=False)\n","tdata_sub.shape\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiRuVW5LScpI","colab_type":"code","colab":{}},"source":["tst"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwhFNtfmScpM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}