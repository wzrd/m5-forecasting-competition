{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/data_science/lib/python3.8/site-packages/lightgbm/__init__.py:42: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  warnings.warn(\"Starting from version 2.2.1, the library file in distribution wheels for macOS \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "from pylab import rcParams\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set()\n",
    "\n",
    "Reduced = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_var():\n",
    "    print(\"Size of Variables\")\n",
    "    _vars = globals().items()\n",
    "    for var, obj in _vars:\n",
    "        print(var, sys.getsizeof(obj))\n",
    "    \n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def read_data():\n",
    "    print('Reading files...')\n",
    "    INPUT_DIR = '../input/m5-forecasting-accuracy'\n",
    "    calendar = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\n",
    "    sales_train_validation = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\n",
    "    sell_prices = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')\n",
    "    sample_submission = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "    \n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "    sell_prices = reduce_mem_usage(sell_prices)\n",
    "    sample_submission = reduce_mem_usage(sample_submission)\n",
    "\n",
    "    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n",
    "    print('Sample_submission has {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
    "    return calendar, sell_prices, sales_train_validation, sample_submission\n",
    "\n",
    "def display_missing(df):    \n",
    "    for col in df.columns.tolist():  \n",
    "        if df[col].isnull().sum() != 0:\n",
    "            print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n",
    "    print('\\n')\n",
    "    \n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null)+1, index=not_null.index)\n",
    "\n",
    "    return df    \n",
    "\n",
    "def weight_calc(data,product):\n",
    "    \n",
    "    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n",
    "\n",
    "    sales_train_val = pd.read_csv('data/sales_train_validation.csv')\n",
    "\n",
    "    d_name = ['d_' + str(i+1) for i in range(1913)]\n",
    "\n",
    "    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n",
    "\n",
    "    # calculate the start position(first non-zero demand observed date) for each item  \n",
    "    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))\n",
    "\n",
    "    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n",
    "\n",
    "    flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))<1\n",
    "\n",
    "    sales_train_val = np.where(flag,np.nan,sales_train_val)\n",
    "\n",
    "    # denominator of RMSSE / RMSSE \n",
    "    weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1913-start_no)\n",
    "\n",
    "    # calculate the sales amount for each item/level\n",
    "    df_tmp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n",
    "    df_tmp['amount'] = df_tmp['demand'] * df_tmp['sell_price']\n",
    "    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n",
    "    df_tmp = df_tmp[product.id].values\n",
    "    \n",
    "    weight2 = weight_mat_csr * df_tmp \n",
    "\n",
    "    weight2 = weight2/np.sum(weight2)\n",
    "\n",
    "    del sales_train_val,df_tmp,flag,start_no\n",
    "    gc.collect()\n",
    "    \n",
    "    return weight1, weight2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loading data and computing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 94.01 Mb (78.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "sales_train_validation = pd.read_csv('data/sales_train_validation.csv')\n",
    "\n",
    "sales_train_validation = encode_categorical(\n",
    "    sales_train_validation, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "weight_mat = np.c_[np.ones([30490,1]).astype(np.int8), # level 1\n",
    "                   pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   np.identity(30490).astype(np.int8) #item :level 12\n",
    "                   ].T\n",
    "\n",
    "weight_mat_csr = csr_matrix(weight_mat)\n",
    "del weight_mat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading the data...')\n",
    "DATA_GRID_INPUT_DIR = '.'\n",
    "\n",
    "data = pd.read_pickle(f'{DATA_GRID_INPUT_DIR}/m5_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1, weight2 = weight_calc(data,product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Saving to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('weight1.pkl', 'wb') as f:\n",
    "    pickle.dump(weight1, f)\n",
    "    \n",
    "with open('weight2.pkl', 'wb') as f:\n",
    "    pickle.dump(weight2, f)\n",
    "    \n",
    "with open('weight_mat_csr.pkl', 'wb') as f:\n",
    "    pickle.dump(weight_mat_csr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
