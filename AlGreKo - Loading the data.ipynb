{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/data_science/lib/python3.8/site-packages/lightgbm/__init__.py:42: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  warnings.warn(\"Starting from version 2.2.1, the library file in distribution wheels for macOS \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "from pylab import rcParams\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set()\n",
    "\n",
    "Reduced = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_var():\n",
    "    print(\"Size of Variables\")\n",
    "    _vars = globals().items()\n",
    "    for var, obj in _vars:\n",
    "        print(var, sys.getsizeof(obj))\n",
    "    \n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def read_data():\n",
    "    print('Reading files...')\n",
    "    INPUT_DIR = 'data'\n",
    "    calendar = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\n",
    "    sales_train_validation = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\n",
    "    sell_prices = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')\n",
    "    sample_submission = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "    \n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "    sell_prices = reduce_mem_usage(sell_prices)\n",
    "    sample_submission = reduce_mem_usage(sample_submission)\n",
    "\n",
    "    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n",
    "    print('Sample_submission has {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
    "    return calendar, sell_prices, sales_train_validation, sample_submission\n",
    "\n",
    "def display_missing(df):    \n",
    "    for col in df.columns.tolist():  \n",
    "        if df[col].isnull().sum() != 0:\n",
    "            print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n",
    "    print('\\n')\n",
    "    \n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null)+1, index=not_null.index)\n",
    "\n",
    "    return df    \n",
    "\n",
    "def melt(sales_train_validation, submission, reduced):\n",
    "    # melt sales data, get it ready for training\n",
    "    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "\n",
    "    # get product table.\n",
    "    product = sales_train_validation[id_columns]\n",
    "    \n",
    "    if reduced == True :\n",
    "        sales_train_validation = sales_train_validation.drop([f\"d_{d}\" for d in range(1, 719)], axis = 1).melt( id_vars=id_columns, var_name=\"d\", value_name=\"demand\" )\n",
    "    else:\n",
    "        sales_train_validation = sales_train_validation.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "\n",
    "    sales_train_validation = reduce_mem_usage(sales_train_validation, verbose=False)\n",
    "\n",
    "    # separate test dataframes.\n",
    "    vals = submission[submission[\"id\"].str.endswith(\"validation\")]\n",
    "    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n",
    "\n",
    "    # change column names.\n",
    "    vals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + 28)]\n",
    "    evals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + 28)]\n",
    "\n",
    "    # merge with product table\n",
    "    evals[\"id\"] = evals[\"id\"].str.replace(\"_evaluation\", \"_validation\")\n",
    "    vals = vals.merge(product, how=\"left\", on=\"id\") # Adds info [item_id;dept_id;cat_id;store_id;state_id]\n",
    "    evals = evals.merge(product, how=\"left\", on=\"id\")\n",
    "    evals[\"id\"] = evals[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n",
    "    \n",
    "    del product\n",
    "    gc.collect()\n",
    "    \n",
    "    vals = vals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "\n",
    "    sales_train_validation[\"part\"] = \"train\"\n",
    "    vals[\"part\"] = \"validation\"\n",
    "    evals[\"part\"] = \"evaluation\"\n",
    "\n",
    "    data = pd.concat([sales_train_validation, vals, evals], axis=0)\n",
    "    del sales_train_validation, vals, evals\n",
    "\n",
    "    # delete evaluation for now.\n",
    "    data = data[data[\"part\"] != \"evaluation\"]\n",
    "    gc.collect()\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_d(df):\n",
    "    return df[\"d\"].str.extract(r\"d_(\\d+)\").astype(np.int16)\n",
    "\n",
    "\n",
    "def merge_calendar(data, calendar):\n",
    "    #calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\", 'event_name_1', 'event_name_2'], axis=1)\n",
    "    return data.merge(calendar, how=\"left\", on=\"d\").assign(d=extract_d)\n",
    "\n",
    "\n",
    "def merge_sell_prices(data, sell_prices):\n",
    "    return data.merge(sell_prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load csv files to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Mem. usage decreased to 95.00 Mb (78.7% reduction)\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Mem. usage decreased to  2.09 Mb (84.5% reduction)\n",
      "Calendar has 1969 rows and 14 columns\n",
      "Sales train validation has 30490 rows and 1919 columns\n",
      "Sell prices has 6841121 rows and 4 columns\n",
      "Sample_submission has 60980 rows and 29 columns\n"
     ]
    }
   ],
   "source": [
    "#Load csv files to variables\n",
    "calendar, sell_prices, sales_train_validation, sample_submission = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.12 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 94.01 Mb (1.0% reduction)\n",
      "Mem. usage decreased to 45.67 Mb (65.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "calendar=reduce_mem_usage(calendar)\n",
    "\n",
    "sales_train_validation = encode_categorical(\n",
    "    sales_train_validation, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "sell_prices = encode_categorical(sell_prices, [\"item_id\", \"store_id\"]\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "product2 = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Melt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2257.58 Mb (0.0% reduction)\n",
      "Time-related data\n",
      "Price-related data\n",
      "Mem. usage decreased to 5248.87 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "data = melt(sales_train_validation, sample_submission ,Reduced)\n",
    "del sales_train_validation\n",
    "del sample_submission\n",
    "\n",
    "data = reduce_mem_usage(data)\n",
    "data1 = data.loc[:len(data[\"part\"])/2]\n",
    "data2 = data.loc[len(data[\"part\"])/2:]\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "print(\"Time-related data\")\n",
    "data1 = merge_calendar(data1, calendar)\n",
    "data2 = merge_calendar(data2, calendar)\n",
    "del calendar\n",
    "gc.collect()\n",
    "\n",
    "frames = [data1, data2]\n",
    "data = pd.concat(frames)\n",
    "del data1,data2,frames\n",
    "gc.collect()\n",
    "\n",
    "print(\"Price-related data\")\n",
    "data = merge_sell_prices(data, sell_prices)\n",
    "del sell_prices\n",
    "gc.collect()\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Combine SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 5135.99 Mb (0.0% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions = [\n",
    "    (data['state_id'] == 1),\n",
    "    (data['state_id'] == 2),\n",
    "    (data['state_id'] == 3)]\n",
    "choices = [data['snap_CA'], data['snap_TX'], data['snap_WI']]\n",
    "data['snap'] = np.select(conditions, choices)\n",
    "data =data.drop(['snap_CA', 'snap_TX', 'snap_WI'], axis=1)\n",
    "data = reduce_mem_usage(data)\n",
    "del conditions, choices\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Remove rows with NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove na values of sale_price\n",
    "data.dropna(subset=['sell_price'],inplace=True)\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Add Calendar Feutures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 4292.15 Mb (2.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "data['woy']   = data['wm_yr_wk']%100\n",
    "data['fyear'] = (data['wm_yr_wk']//100)%100\n",
    "data['week_of_total'] = np.where( data['fyear'] <14, (data['woy'] + (data['fyear']- 11) * 52 ), ( 157+ data['woy']  + ((data['fyear']- 14) * 52)) ) \n",
    "data[\"is_weekend\"] = data[\"wday\"].isin([1, 2]).astype(np.int8)\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Setting data types for optimal memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = data['id'].astype('category') \n",
    "data['part'] = data['part'].astype('category')\n",
    "data['date'] = data['date'].astype('datetime64[ns]')\n",
    "data['weekday'] = data['weekday'].astype('category')\n",
    "data['event_name_1'] = data['event_name_1'].astype('category')\n",
    "data['event_type_1'] = data['event_type_1'].astype('category')\n",
    "data['event_name_2'] = data['event_name_2'].astype('category')\n",
    "data['event_type_2'] = data['event_type_2'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>demand</th>\n",
       "      <th>part</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>snap</th>\n",
       "      <th>woy</th>\n",
       "      <th>fyear</th>\n",
       "      <th>week_of_total</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>1445</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.459961</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>1446</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.559570</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>1447</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.169922</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
       "      <td>1449</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.980469</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
       "      <td>1452</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  item_id  dept_id  cat_id  store_id  \\\n",
       "7   HOBBIES_1_008_CA_1_validation     1445        4       2         1   \n",
       "8   HOBBIES_1_009_CA_1_validation     1446        4       2         1   \n",
       "9   HOBBIES_1_010_CA_1_validation     1447        4       2         1   \n",
       "11  HOBBIES_1_012_CA_1_validation     1449        4       2         1   \n",
       "14  HOBBIES_1_015_CA_1_validation     1452        4       2         1   \n",
       "\n",
       "    state_id  d  demand   part       date  wm_yr_wk   weekday  wday  month  \\\n",
       "7          1  1      12  train 2011-01-29     11101  Saturday     1      1   \n",
       "8          1  1       2  train 2011-01-29     11101  Saturday     1      1   \n",
       "9          1  1       0  train 2011-01-29     11101  Saturday     1      1   \n",
       "11         1  1       0  train 2011-01-29     11101  Saturday     1      1   \n",
       "14         1  1       4  train 2011-01-29     11101  Saturday     1      1   \n",
       "\n",
       "    year event_name_1 event_type_1 event_name_2 event_type_2  sell_price  \\\n",
       "7   2011          NaN          NaN          NaN          NaN    0.459961   \n",
       "8   2011          NaN          NaN          NaN          NaN    1.559570   \n",
       "9   2011          NaN          NaN          NaN          NaN    3.169922   \n",
       "11  2011          NaN          NaN          NaN          NaN    5.980469   \n",
       "14  2011          NaN          NaN          NaN          NaN    0.700195   \n",
       "\n",
       "    snap  woy  fyear  week_of_total  is_weekend  \n",
       "7      0    1     11              1           1  \n",
       "8      0    1     11              1           1  \n",
       "9      0    1     11              1           1  \n",
       "11     0    1     11              1           1  \n",
       "14     0    1     11              1           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data usage: Index            375.053424\n",
      "id                97.688416\n",
      "item_id           93.763356\n",
      "dept_id           46.881678\n",
      "cat_id            46.881678\n",
      "store_id          46.881678\n",
      "state_id          46.881678\n",
      "d                 93.763356\n",
      "demand            93.763356\n",
      "part              46.881887\n",
      "date             375.053424\n",
      "wm_yr_wk          93.763356\n",
      "weekday           46.882447\n",
      "wday              46.881678\n",
      "month             46.881678\n",
      "year              93.763356\n",
      "event_name_1      46.885011\n",
      "event_type_1      46.882099\n",
      "event_name_2      46.882111\n",
      "event_type_2      46.881889\n",
      "sell_price        93.763356\n",
      "snap              46.881678\n",
      "woy               46.881678\n",
      "fyear             46.881678\n",
      "week_of_total     93.763356\n",
      "is_weekend        46.881678\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#print('Data usage: {} GB'.format(data.memory_usage().sum() / 10**9))\n",
    "print('Data usage: {}'.format(data.memory_usage(deep=True) / 10**6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving the data...\")\n",
    "data.to_pickle('m5_data.pkl')\n",
    "# data.to_hdf('m5_data.h5', key='df', mode='w', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
